architecture: "gpt2"
size: "medium"
max_seq_len:
vocab_size: 50257
n_positions: 1024
n_embed: 1024
n_layer: 12
n_head: 12
n_inner: None
activation_function: "gelu_new"
resid_pdrop: 0.1
embd_pdrop: 0.1
attn_pdrop: 0.1
layer_norm_epsilon: 1e-05
initializer_range: 0.02
summary_type: "cls_index"
summary_use_proj: True
summary_activation: None
summary_proj_to_labels: True
summary_first_dropout: 0.1
scale_attn_weights: True
use_cache: True
bos_token_id: 50256
scale_attn_by_inverse_layer_idx: False
reorder_and_upcast_attn: False
valid_special_input_len: 500
pad_token_id: 50256